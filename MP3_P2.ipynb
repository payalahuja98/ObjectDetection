{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "MP3_P2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/payalahuja98/ObjectDetection/blob/main/MP3_P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqz7po6DTC7d",
        "outputId": "4a0e9a44-5982-4c69-a4e5-afe3b2014dbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EWBnbFKUUxG",
        "outputId": "948dd075-764a-4b20-824e-59bd98c70b9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check cuda version on colab and run the following PyTorch installation commands respectively.\n",
        "!nvcc --version\n",
        "\n",
        "\n",
        "# If CUDA 10.1 is installed \n",
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 24kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 29.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "185vbdDWS8-s"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/FA2020/CS498/assignment3_p2_starterkit')\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "\n",
        "from resnet_yolo import resnet50\n",
        "from yolo_loss import YoloLoss\n",
        "from dataset import VocDetectorDataset\n",
        "from eval_voc import evaluate\n",
        "from predict import predict_image\n",
        "from config import VOC_CLASSES, COLORS\n",
        "from kaggle_submission import output_submission_csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR9J132YWyt4",
        "outputId": "3a8e0bf7-994a-4bf9-fb49-dd225f9da7ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import shutil \n",
        "shutil.copyfile(\"VOCtrainval_06-Nov-2007.tar\", \"/content/VOCtrainval_06-Nov-2007.tar\")\n",
        "!tar -xf \"/content/VOCtrainval_06-Nov-2007.tar\" -C \"/content/\" \n",
        "shutil.move(\"/content/VOCdevkit/\", \"/content/VOCdevkit_2007\")\n",
        "\n",
        "shutil.copyfile(\"VOCtest_06-Nov-2007.tar\", \"/content/VOCtest_06-Nov-2007.tar\")\n",
        "!tar -xf \"/content/VOCtest_06-Nov-2007.tar\" -C \"/content/\" \n",
        "shutil.move(\"/content/VOCdevkit/VOC2007\", \"/content/VOCdevkit_2007/VOC2007test\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/VOCdevkit_2007/VOC2007test'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqoMAy5LS8-w"
      },
      "source": [
        "# Assignment3 Part2: Yolo Detection\n",
        "\n",
        "We provide you a Yolo Detection network implementation, which is not finished. You are asked to complete the implementation by writing the loss function.\n",
        "\n",
        "## What to do\n",
        "You are asked to implement the loss function in ```yolo_loss.py```. You can use ```yolo_loss_debug_tool.ipynb``` to help you debug.\n",
        "\n",
        "## What to submit\n",
        "See the submission template for what to submit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YevFKZFhS8-w"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS3YUKptS8-x"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22HX6mVKS8-2"
      },
      "source": [
        "# YOLO network hyperparameters\n",
        "B = 2  # number of bounding box predictions per cell\n",
        "S = 14  # width/height of network output grid (larger than 7x7 from paper since we use a different network)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qdCrSm_S8-5"
      },
      "source": [
        "To implement Yolo we will rely on a pretrained classifier as the backbone for our detection network. PyTorch offers a variety of models which are pretrained on ImageNet in the [`torchvision.models`](https://pytorch.org/docs/stable/torchvision/models.html) package. In particular, we will use the ResNet50 architecture as a base for our detector. This is different from the base architecture in the Yolo paper and also results in a different output grid size (14x14 instead of 7x7).\n",
        "\n",
        "Models are typically pretrained on ImageNet since the dataset is very large (> 1million images) and widely used. The pretrained model provides a very useful weight initialization for our detector, so that the network is able to learn quickly and effictively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTnx7kCuS8-5",
        "outputId": "0c67bb42-460b-4b62-d46d-42f283975b68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "load_network_path = 'best_detector.pth'\n",
        "pretrained = True\n",
        "\n",
        "# use to load a previously trained network\n",
        "if load_network_path is not None:\n",
        "    print('Loading saved network from {}'.format(load_network_path))\n",
        "    net = resnet50().to(device)\n",
        "    net.load_state_dict(torch.load(load_network_path))\n",
        "else:\n",
        "    print('Load pre-trained model')\n",
        "    net = resnet50(pretrained=pretrained).to(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading saved network from best_detector.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK6Qc5cCS8-8"
      },
      "source": [
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "batch_size = 24\n",
        "\n",
        "# Yolo loss component coefficients (as given in Yolo v1 paper)\n",
        "lambda_coord = 5\n",
        "lambda_noobj = 0.5"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4waLVWRJS8-_"
      },
      "source": [
        "criterion = YoloLoss(S, B, lambda_coord, lambda_noobj)\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5x1drBIS8_E"
      },
      "source": [
        "## Reading Pascal Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2utLL5IS8_E"
      },
      "source": [
        "Since Pascal is a small dataset (5000 in train+val) we have combined the train and val splits to train our detector. This is not typically a good practice, but we will make an exception in this case to be able to get reasonable detection results with a comparatively small object detection dataset.\n",
        "\n",
        "The train dataset loader also using a variety of data augmentation techniques including random shift, scaling, crop, and flips. Data augmentation is slightly more complicated for detection dataset since the bounding box annotations must be kept consistent through the transformations.\n",
        "\n",
        "Since the output of the dector network we train is an SxSx(B*5+C), we use an encoder to convert the original bounding box coordinates into relative grid bounding box coordinates corresponding to the the expected output. We also use a decoder which allows us to convert the opposite direction into image coordinate bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBbR9hF6S8_F",
        "outputId": "23fe0281-a279-45a8-89b8-bbccb6d3c761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "file_root_train = '/content/VOCdevkit_2007/VOC2007/JPEGImages/'\n",
        "annotation_file_train = 'voc2007.txt'\n",
        "\n",
        "train_dataset = VocDetectorDataset(root_img_dir=file_root_train,dataset_file=annotation_file_train,train=True, S=S)\n",
        "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "print('Loaded %d train images' % len(train_dataset))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing dataset\n",
            "Loaded 5011 train images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrrYXM9OS8_L",
        "outputId": "920de448-bc78-446a-ddcc-20e6ca52aa5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "file_root_test = '/content/VOCdevkit_2007/VOC2007test/JPEGImages/'\n",
        "annotation_file_test = 'voc2007test.txt'\n",
        "\n",
        "test_dataset = VocDetectorDataset(root_img_dir=file_root_test,dataset_file=annotation_file_test,train=False, S=S)\n",
        "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=4)\n",
        "print('Loaded %d test images' % len(test_dataset))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing dataset\n",
            "Loaded 4950 test images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa8tllXzS8_N"
      },
      "source": [
        "## Train detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eNdjMSiS8_O",
        "outputId": "cbaf1852-b70e-4353-e94e-a79032dd95fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "best_test_loss = np.inf\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    \n",
        "    # Update learning rate late in training\n",
        "    if epoch == 30 or epoch == 40:\n",
        "        learning_rate /= 10.0\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = learning_rate\n",
        "    \n",
        "    print('\\n\\nStarting epoch %d / %d' % (epoch + 1, num_epochs))\n",
        "    print('Learning Rate for this epoch: {}'.format(learning_rate))\n",
        "    \n",
        "    total_loss = 0.\n",
        "    \n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        images, target = images.to(device), target.to(device)\n",
        "        \n",
        "        pred = net(images)\n",
        "        loss = criterion(pred,target)\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (i+1) % 50 == 0:\n",
        "            print('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f, average_loss: %.4f'\n",
        "                  % (epoch+1, num_epochs, i+1, len(train_loader), loss.item(), total_loss / (i+1)))\n",
        "    \n",
        "    # evaluate the network on the test data\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.0\n",
        "        net.eval()\n",
        "        for i, (images, target) in enumerate(test_loader):\n",
        "            images, target = images.to(device), target.to(device)\n",
        "\n",
        "            pred = net(images)\n",
        "            loss = criterion(pred,target)\n",
        "            test_loss += loss.item()\n",
        "        test_loss /= len(test_loader)\n",
        "    \n",
        "    if best_test_loss > test_loss:\n",
        "        best_test_loss = test_loss\n",
        "        print('Updating best test loss: %.5f' % best_test_loss)\n",
        "        torch.save(net.state_dict(),'best_detector.pth')\n",
        "\n",
        "    torch.save(net.state_dict(),'detector.pth')\n",
        "    \n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Starting epoch 1 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [1/80], Iter [50/209] Loss: 8.7401, average_loss: 23.5215\n",
            "Epoch [1/80], Iter [100/209] Loss: 5.5450, average_loss: 15.3190\n",
            "Epoch [1/80], Iter [150/209] Loss: 4.6023, average_loss: 12.2268\n",
            "Epoch [1/80], Iter [200/209] Loss: 5.4011, average_loss: 10.5662\n",
            "Updating best test loss: 5.16589\n",
            "\n",
            "\n",
            "Starting epoch 2 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [2/80], Iter [50/209] Loss: 5.4800, average_loss: 5.0991\n",
            "Epoch [2/80], Iter [100/209] Loss: 5.5930, average_loss: 4.9781\n",
            "Epoch [2/80], Iter [150/209] Loss: 4.0764, average_loss: 4.8910\n",
            "Epoch [2/80], Iter [200/209] Loss: 5.6979, average_loss: 4.8286\n",
            "Updating best test loss: 4.62810\n",
            "\n",
            "\n",
            "Starting epoch 3 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [3/80], Iter [50/209] Loss: 3.5331, average_loss: 4.3032\n",
            "Epoch [3/80], Iter [100/209] Loss: 6.4804, average_loss: 4.3298\n",
            "Epoch [3/80], Iter [150/209] Loss: 4.0740, average_loss: 4.3087\n",
            "Epoch [3/80], Iter [200/209] Loss: 3.3218, average_loss: 4.3135\n",
            "Updating best test loss: 4.20893\n",
            "\n",
            "\n",
            "Starting epoch 4 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [4/80], Iter [50/209] Loss: 3.7339, average_loss: 3.9117\n",
            "Epoch [4/80], Iter [100/209] Loss: 3.4180, average_loss: 3.9725\n",
            "Epoch [4/80], Iter [150/209] Loss: 3.5918, average_loss: 3.9718\n",
            "Epoch [4/80], Iter [200/209] Loss: 4.2367, average_loss: 3.9847\n",
            "Updating best test loss: 3.93480\n",
            "\n",
            "\n",
            "Starting epoch 5 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [5/80], Iter [50/209] Loss: 3.8203, average_loss: 3.7956\n",
            "Epoch [5/80], Iter [100/209] Loss: 3.2927, average_loss: 3.7072\n",
            "Epoch [5/80], Iter [150/209] Loss: 4.5665, average_loss: 3.6858\n",
            "Epoch [5/80], Iter [200/209] Loss: 3.7144, average_loss: 3.6875\n",
            "Updating best test loss: 3.72239\n",
            "\n",
            "\n",
            "Starting epoch 6 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [6/80], Iter [50/209] Loss: 3.6981, average_loss: 3.4387\n",
            "Epoch [6/80], Iter [100/209] Loss: 3.7301, average_loss: 3.4798\n",
            "Epoch [6/80], Iter [150/209] Loss: 2.8374, average_loss: 3.4325\n",
            "Epoch [6/80], Iter [200/209] Loss: 3.1101, average_loss: 3.4962\n",
            "Updating best test loss: 3.59947\n",
            "\n",
            "\n",
            "Starting epoch 7 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [7/80], Iter [50/209] Loss: 3.1282, average_loss: 3.3768\n",
            "Epoch [7/80], Iter [100/209] Loss: 4.6366, average_loss: 3.3681\n",
            "Epoch [7/80], Iter [150/209] Loss: 3.0971, average_loss: 3.3193\n",
            "Epoch [7/80], Iter [200/209] Loss: 2.9757, average_loss: 3.3360\n",
            "Updating best test loss: 3.50636\n",
            "\n",
            "\n",
            "Starting epoch 8 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [8/80], Iter [50/209] Loss: 3.2900, average_loss: 3.1027\n",
            "Epoch [8/80], Iter [100/209] Loss: 4.0314, average_loss: 3.1747\n",
            "Epoch [8/80], Iter [150/209] Loss: 3.0420, average_loss: 3.2271\n",
            "Epoch [8/80], Iter [200/209] Loss: 4.8093, average_loss: 3.2412\n",
            "Updating best test loss: 3.39885\n",
            "\n",
            "\n",
            "Starting epoch 9 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [9/80], Iter [50/209] Loss: 3.4073, average_loss: 3.1585\n",
            "Epoch [9/80], Iter [100/209] Loss: 2.9417, average_loss: 3.0946\n",
            "Epoch [9/80], Iter [150/209] Loss: 3.3134, average_loss: 3.1108\n",
            "Epoch [9/80], Iter [200/209] Loss: 2.7153, average_loss: 3.0899\n",
            "Updating best test loss: 3.35500\n",
            "\n",
            "\n",
            "Starting epoch 10 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [10/80], Iter [50/209] Loss: 3.2906, average_loss: 3.0165\n",
            "Epoch [10/80], Iter [100/209] Loss: 3.2844, average_loss: 2.9715\n",
            "Epoch [10/80], Iter [150/209] Loss: 2.7676, average_loss: 2.9976\n",
            "Epoch [10/80], Iter [200/209] Loss: 2.1131, average_loss: 2.9688\n",
            "Updating best test loss: 3.30809\n",
            "\n",
            "\n",
            "Starting epoch 11 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [11/80], Iter [50/209] Loss: 2.5986, average_loss: 2.8125\n",
            "Epoch [11/80], Iter [100/209] Loss: 2.4491, average_loss: 2.8275\n",
            "Epoch [11/80], Iter [150/209] Loss: 3.2209, average_loss: 2.8881\n",
            "Epoch [11/80], Iter [200/209] Loss: 2.8877, average_loss: 2.8936\n",
            "Updating best test loss: 3.20032\n",
            "\n",
            "\n",
            "Starting epoch 12 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [12/80], Iter [50/209] Loss: 2.8930, average_loss: 2.8902\n",
            "Epoch [12/80], Iter [100/209] Loss: 2.9500, average_loss: 2.8583\n",
            "Epoch [12/80], Iter [150/209] Loss: 2.4449, average_loss: 2.8062\n",
            "Epoch [12/80], Iter [200/209] Loss: 2.7709, average_loss: 2.8080\n",
            "Updating best test loss: 3.17244\n",
            "\n",
            "\n",
            "Starting epoch 13 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [13/80], Iter [50/209] Loss: 2.3925, average_loss: 2.6797\n",
            "Epoch [13/80], Iter [100/209] Loss: 3.2157, average_loss: 2.7501\n",
            "Epoch [13/80], Iter [150/209] Loss: 2.8582, average_loss: 2.7436\n",
            "Epoch [13/80], Iter [200/209] Loss: 2.4115, average_loss: 2.7360\n",
            "Updating best test loss: 3.14594\n",
            "\n",
            "\n",
            "Starting epoch 14 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [14/80], Iter [50/209] Loss: 2.2495, average_loss: 2.6673\n",
            "Epoch [14/80], Iter [100/209] Loss: 2.9805, average_loss: 2.6596\n",
            "Epoch [14/80], Iter [150/209] Loss: 3.9630, average_loss: 2.6838\n",
            "Epoch [14/80], Iter [200/209] Loss: 1.8654, average_loss: 2.6590\n",
            "Updating best test loss: 3.08825\n",
            "\n",
            "\n",
            "Starting epoch 15 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [15/80], Iter [50/209] Loss: 2.8063, average_loss: 2.6145\n",
            "Epoch [15/80], Iter [100/209] Loss: 2.6603, average_loss: 2.6224\n",
            "Epoch [15/80], Iter [150/209] Loss: 2.0014, average_loss: 2.6171\n",
            "Epoch [15/80], Iter [200/209] Loss: 2.6037, average_loss: 2.5965\n",
            "Updating best test loss: 3.07340\n",
            "\n",
            "\n",
            "Starting epoch 16 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [16/80], Iter [50/209] Loss: 2.4039, average_loss: 2.5540\n",
            "Epoch [16/80], Iter [100/209] Loss: 2.2497, average_loss: 2.5694\n",
            "Epoch [16/80], Iter [150/209] Loss: 2.5126, average_loss: 2.5999\n",
            "Epoch [16/80], Iter [200/209] Loss: 2.4340, average_loss: 2.5802\n",
            "Updating best test loss: 3.01768\n",
            "\n",
            "\n",
            "Starting epoch 17 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [17/80], Iter [50/209] Loss: 2.2387, average_loss: 2.5431\n",
            "Epoch [17/80], Iter [100/209] Loss: 1.9247, average_loss: 2.5917\n",
            "Epoch [17/80], Iter [150/209] Loss: 2.3719, average_loss: 2.5493\n",
            "Epoch [17/80], Iter [200/209] Loss: 2.1720, average_loss: 2.5322\n",
            "Updating best test loss: 2.97342\n",
            "\n",
            "\n",
            "Starting epoch 18 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [18/80], Iter [50/209] Loss: 1.9233, average_loss: 2.3458\n",
            "Epoch [18/80], Iter [100/209] Loss: 2.8407, average_loss: 2.3966\n",
            "Epoch [18/80], Iter [150/209] Loss: 2.4774, average_loss: 2.4104\n",
            "Epoch [18/80], Iter [200/209] Loss: 1.8632, average_loss: 2.4043\n",
            "Updating best test loss: 2.96944\n",
            "\n",
            "\n",
            "Starting epoch 19 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [19/80], Iter [50/209] Loss: 2.7487, average_loss: 2.4306\n",
            "Epoch [19/80], Iter [100/209] Loss: 2.3146, average_loss: 2.3928\n",
            "Epoch [19/80], Iter [150/209] Loss: 2.6604, average_loss: 2.3863\n",
            "Epoch [19/80], Iter [200/209] Loss: 2.6868, average_loss: 2.4136\n",
            "Updating best test loss: 2.94077\n",
            "\n",
            "\n",
            "Starting epoch 20 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [20/80], Iter [50/209] Loss: 2.5900, average_loss: 2.3988\n",
            "Epoch [20/80], Iter [100/209] Loss: 3.3281, average_loss: 2.3667\n",
            "Epoch [20/80], Iter [150/209] Loss: 2.1942, average_loss: 2.3667\n",
            "Epoch [20/80], Iter [200/209] Loss: 2.4897, average_loss: 2.3648\n",
            "\n",
            "\n",
            "Starting epoch 21 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [21/80], Iter [50/209] Loss: 3.5110, average_loss: 2.3321\n",
            "Epoch [21/80], Iter [100/209] Loss: 3.5398, average_loss: 2.2831\n",
            "Epoch [21/80], Iter [150/209] Loss: 1.9927, average_loss: 2.2890\n",
            "Epoch [21/80], Iter [200/209] Loss: 2.1540, average_loss: 2.3259\n",
            "\n",
            "\n",
            "Starting epoch 22 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [22/80], Iter [50/209] Loss: 2.6779, average_loss: 2.3350\n",
            "Epoch [22/80], Iter [100/209] Loss: 2.7523, average_loss: 2.3279\n",
            "Epoch [22/80], Iter [150/209] Loss: 2.1678, average_loss: 2.3061\n",
            "Epoch [22/80], Iter [200/209] Loss: 2.0924, average_loss: 2.3044\n",
            "Updating best test loss: 2.91280\n",
            "\n",
            "\n",
            "Starting epoch 23 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [23/80], Iter [50/209] Loss: 2.1004, average_loss: 2.2559\n",
            "Epoch [23/80], Iter [100/209] Loss: 2.7157, average_loss: 2.2269\n",
            "Epoch [23/80], Iter [150/209] Loss: 2.4766, average_loss: 2.2251\n",
            "Epoch [23/80], Iter [200/209] Loss: 1.8648, average_loss: 2.2387\n",
            "Updating best test loss: 2.89457\n",
            "\n",
            "\n",
            "Starting epoch 24 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [24/80], Iter [50/209] Loss: 2.7057, average_loss: 2.1178\n",
            "Epoch [24/80], Iter [100/209] Loss: 1.6505, average_loss: 2.1845\n",
            "Epoch [24/80], Iter [150/209] Loss: 1.3317, average_loss: 2.1989\n",
            "Epoch [24/80], Iter [200/209] Loss: 2.2807, average_loss: 2.2048\n",
            "Updating best test loss: 2.84938\n",
            "\n",
            "\n",
            "Starting epoch 25 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [25/80], Iter [50/209] Loss: 2.5229, average_loss: 2.2143\n",
            "Epoch [25/80], Iter [100/209] Loss: 2.2929, average_loss: 2.1913\n",
            "Epoch [25/80], Iter [150/209] Loss: 3.2282, average_loss: 2.1884\n",
            "Epoch [25/80], Iter [200/209] Loss: 1.9743, average_loss: 2.1751\n",
            "\n",
            "\n",
            "Starting epoch 26 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [26/80], Iter [50/209] Loss: 2.5249, average_loss: 2.0914\n",
            "Epoch [26/80], Iter [100/209] Loss: 1.5079, average_loss: 2.0673\n",
            "Epoch [26/80], Iter [150/209] Loss: 1.7367, average_loss: 2.1031\n",
            "Epoch [26/80], Iter [200/209] Loss: 1.8755, average_loss: 2.1013\n",
            "\n",
            "\n",
            "Starting epoch 27 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [27/80], Iter [50/209] Loss: 1.6574, average_loss: 2.0165\n",
            "Epoch [27/80], Iter [100/209] Loss: 2.1885, average_loss: 2.0570\n",
            "Epoch [27/80], Iter [150/209] Loss: 3.3941, average_loss: 2.1057\n",
            "Epoch [27/80], Iter [200/209] Loss: 2.5060, average_loss: 2.1242\n",
            "\n",
            "\n",
            "Starting epoch 28 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [28/80], Iter [50/209] Loss: 2.2073, average_loss: 2.0633\n",
            "Epoch [28/80], Iter [100/209] Loss: 2.8057, average_loss: 2.1056\n",
            "Epoch [28/80], Iter [150/209] Loss: 1.4661, average_loss: 2.0731\n",
            "Epoch [28/80], Iter [200/209] Loss: 2.2524, average_loss: 2.0806\n",
            "Updating best test loss: 2.84581\n",
            "\n",
            "\n",
            "Starting epoch 29 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [29/80], Iter [50/209] Loss: 2.2090, average_loss: 2.0351\n",
            "Epoch [29/80], Iter [100/209] Loss: 2.2830, average_loss: 2.0379\n",
            "Epoch [29/80], Iter [150/209] Loss: 2.3317, average_loss: 2.0680\n",
            "Epoch [29/80], Iter [200/209] Loss: 1.5084, average_loss: 2.0730\n",
            "Updating best test loss: 2.83792\n",
            "\n",
            "\n",
            "Starting epoch 30 / 80\n",
            "Learning Rate for this epoch: 0.001\n",
            "Epoch [30/80], Iter [50/209] Loss: 2.1689, average_loss: 2.0930\n",
            "Epoch [30/80], Iter [100/209] Loss: 2.9823, average_loss: 2.0605\n",
            "Epoch [30/80], Iter [150/209] Loss: 1.5295, average_loss: 2.0299\n",
            "Epoch [30/80], Iter [200/209] Loss: 2.1677, average_loss: 2.0466\n",
            "\n",
            "\n",
            "Starting epoch 31 / 80\n",
            "Learning Rate for this epoch: 0.0001\n",
            "Epoch [31/80], Iter [50/209] Loss: 2.4750, average_loss: 2.0386\n",
            "Epoch [31/80], Iter [100/209] Loss: 1.7276, average_loss: 1.9656\n",
            "Epoch [31/80], Iter [150/209] Loss: 1.7960, average_loss: 1.9528\n",
            "Epoch [31/80], Iter [200/209] Loss: 2.3718, average_loss: 1.9354\n",
            "Updating best test loss: 2.75799\n",
            "\n",
            "\n",
            "Starting epoch 32 / 80\n",
            "Learning Rate for this epoch: 0.0001\n",
            "Epoch [32/80], Iter [50/209] Loss: 1.6599, average_loss: 1.9264\n",
            "Epoch [32/80], Iter [100/209] Loss: 2.1477, average_loss: 1.9404\n",
            "Epoch [32/80], Iter [150/209] Loss: 2.7212, average_loss: 1.9045\n",
            "Epoch [32/80], Iter [200/209] Loss: 2.0329, average_loss: 1.8951\n",
            "Updating best test loss: 2.74954\n",
            "\n",
            "\n",
            "Starting epoch 33 / 80\n",
            "Learning Rate for this epoch: 0.0001\n",
            "Epoch [33/80], Iter [50/209] Loss: 2.0474, average_loss: 1.8910\n",
            "Epoch [33/80], Iter [100/209] Loss: 1.5894, average_loss: 1.8736\n",
            "Epoch [33/80], Iter [150/209] Loss: 1.5085, average_loss: 1.8808\n",
            "Epoch [33/80], Iter [200/209] Loss: 1.5039, average_loss: 1.8878\n",
            "Updating best test loss: 2.74158\n",
            "\n",
            "\n",
            "Starting epoch 34 / 80\n",
            "Learning Rate for this epoch: 0.0001\n",
            "Epoch [34/80], Iter [50/209] Loss: 1.2337, average_loss: 1.7665\n",
            "Epoch [34/80], Iter [100/209] Loss: 2.2313, average_loss: 1.8352\n",
            "Epoch [34/80], Iter [150/209] Loss: 2.0353, average_loss: 1.8529\n",
            "Epoch [34/80], Iter [200/209] Loss: 2.2707, average_loss: 1.8441\n",
            "\n",
            "\n",
            "Starting epoch 35 / 80\n",
            "Learning Rate for this epoch: 0.0001\n",
            "Epoch [35/80], Iter [50/209] Loss: 2.8139, average_loss: 1.9157\n",
            "Epoch [35/80], Iter [100/209] Loss: 2.2404, average_loss: 1.9174\n",
            "Epoch [35/80], Iter [150/209] Loss: 1.9040, average_loss: 1.8846\n",
            "Epoch [35/80], Iter [200/209] Loss: 1.5520, average_loss: 1.8683\n",
            "Updating best test loss: 2.72681\n",
            "\n",
            "\n",
            "Starting epoch 36 / 80\n",
            "Learning Rate for this epoch: 0.0001\n",
            "Epoch [36/80], Iter [50/209] Loss: 2.0047, average_loss: 1.9297\n",
            "Epoch [36/80], Iter [100/209] Loss: 1.8076, average_loss: 1.8550\n",
            "Epoch [36/80], Iter [150/209] Loss: 1.5304, average_loss: 1.8696\n",
            "Epoch [36/80], Iter [200/209] Loss: 1.4341, average_loss: 1.8460\n",
            "\n",
            "\n",
            "Starting epoch 37 / 80\n",
            "Learning Rate for this epoch: 0.0001\n",
            "Epoch [37/80], Iter [50/209] Loss: 2.1792, average_loss: 1.9187\n",
            "Epoch [37/80], Iter [100/209] Loss: 1.8311, average_loss: 1.8692\n",
            "Epoch [37/80], Iter [150/209] Loss: 1.8906, average_loss: 1.8684\n",
            "Epoch [37/80], Iter [200/209] Loss: 1.8851, average_loss: 1.8588\n",
            "\n",
            "\n",
            "Starting epoch 38 / 80\n",
            "Learning Rate for this epoch: 0.0001\n",
            "Epoch [38/80], Iter [50/209] Loss: 2.0293, average_loss: 1.8433\n",
            "Epoch [38/80], Iter [100/209] Loss: 1.7127, average_loss: 1.8368\n",
            "Epoch [38/80], Iter [150/209] Loss: 2.6003, average_loss: 1.8242\n",
            "Epoch [38/80], Iter [200/209] Loss: 1.7910, average_loss: 1.8227\n",
            "\n",
            "\n",
            "Starting epoch 39 / 80\n",
            "Learning Rate for this epoch: 0.0001\n",
            "Epoch [39/80], Iter [50/209] Loss: 1.6868, average_loss: 1.8240\n",
            "Epoch [39/80], Iter [100/209] Loss: 2.0758, average_loss: 1.7311\n",
            "Epoch [39/80], Iter [150/209] Loss: 1.9054, average_loss: 1.7786\n",
            "Epoch [39/80], Iter [200/209] Loss: 2.2876, average_loss: 1.7894\n",
            "\n",
            "\n",
            "Starting epoch 40 / 80\n",
            "Learning Rate for this epoch: 0.0001\n",
            "Epoch [40/80], Iter [50/209] Loss: 1.6963, average_loss: 1.8084\n",
            "Epoch [40/80], Iter [100/209] Loss: 1.4291, average_loss: 1.8442\n",
            "Epoch [40/80], Iter [150/209] Loss: 1.7115, average_loss: 1.8234\n",
            "Epoch [40/80], Iter [200/209] Loss: 1.8952, average_loss: 1.8211\n",
            "\n",
            "\n",
            "Starting epoch 41 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [41/80], Iter [50/209] Loss: 1.4555, average_loss: 1.8168\n",
            "Epoch [41/80], Iter [100/209] Loss: 1.4324, average_loss: 1.7721\n",
            "Epoch [41/80], Iter [150/209] Loss: 2.3744, average_loss: 1.8053\n",
            "Epoch [41/80], Iter [200/209] Loss: 1.3541, average_loss: 1.7994\n",
            "Updating best test loss: 2.72208\n",
            "\n",
            "\n",
            "Starting epoch 42 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [42/80], Iter [50/209] Loss: 1.0561, average_loss: 1.7900\n",
            "Epoch [42/80], Iter [100/209] Loss: 1.3638, average_loss: 1.7887\n",
            "Epoch [42/80], Iter [150/209] Loss: 1.4046, average_loss: 1.8006\n",
            "Epoch [42/80], Iter [200/209] Loss: 2.0441, average_loss: 1.7942\n",
            "\n",
            "\n",
            "Starting epoch 43 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [43/80], Iter [50/209] Loss: 2.2541, average_loss: 1.8102\n",
            "Epoch [43/80], Iter [100/209] Loss: 1.6419, average_loss: 1.8271\n",
            "Epoch [43/80], Iter [150/209] Loss: 1.4550, average_loss: 1.7907\n",
            "Epoch [43/80], Iter [200/209] Loss: 2.2430, average_loss: 1.8128\n",
            "Updating best test loss: 2.71671\n",
            "\n",
            "\n",
            "Starting epoch 44 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [44/80], Iter [50/209] Loss: 1.4466, average_loss: 1.8778\n",
            "Epoch [44/80], Iter [100/209] Loss: 1.9771, average_loss: 1.8372\n",
            "Epoch [44/80], Iter [150/209] Loss: 1.5730, average_loss: 1.8017\n",
            "Epoch [44/80], Iter [200/209] Loss: 1.8275, average_loss: 1.8016\n",
            "\n",
            "\n",
            "Starting epoch 45 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [45/80], Iter [50/209] Loss: 2.6573, average_loss: 1.8442\n",
            "Epoch [45/80], Iter [100/209] Loss: 1.4066, average_loss: 1.8130\n",
            "Epoch [45/80], Iter [150/209] Loss: 1.1085, average_loss: 1.7944\n",
            "Epoch [45/80], Iter [200/209] Loss: 1.7648, average_loss: 1.8046\n",
            "\n",
            "\n",
            "Starting epoch 46 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [46/80], Iter [50/209] Loss: 2.0564, average_loss: 1.8089\n",
            "Epoch [46/80], Iter [100/209] Loss: 2.6425, average_loss: 1.8062\n",
            "Epoch [46/80], Iter [150/209] Loss: 2.2299, average_loss: 1.7975\n",
            "Epoch [46/80], Iter [200/209] Loss: 1.5700, average_loss: 1.7879\n",
            "\n",
            "\n",
            "Starting epoch 47 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [47/80], Iter [50/209] Loss: 2.0107, average_loss: 1.7299\n",
            "Epoch [47/80], Iter [100/209] Loss: 1.8412, average_loss: 1.7613\n",
            "Epoch [47/80], Iter [150/209] Loss: 1.4477, average_loss: 1.7927\n",
            "Epoch [47/80], Iter [200/209] Loss: 1.8692, average_loss: 1.8005\n",
            "\n",
            "\n",
            "Starting epoch 48 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [48/80], Iter [50/209] Loss: 2.2754, average_loss: 1.8634\n",
            "Epoch [48/80], Iter [100/209] Loss: 1.6702, average_loss: 1.8545\n",
            "Epoch [48/80], Iter [150/209] Loss: 1.4498, average_loss: 1.8378\n",
            "Epoch [48/80], Iter [200/209] Loss: 1.6625, average_loss: 1.8225\n",
            "\n",
            "\n",
            "Starting epoch 49 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [49/80], Iter [50/209] Loss: 1.6378, average_loss: 1.8510\n",
            "Epoch [49/80], Iter [100/209] Loss: 2.6585, average_loss: 1.8278\n",
            "Epoch [49/80], Iter [150/209] Loss: 1.4164, average_loss: 1.7917\n",
            "Epoch [49/80], Iter [200/209] Loss: 1.9968, average_loss: 1.8017\n",
            "\n",
            "\n",
            "Starting epoch 50 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [50/80], Iter [50/209] Loss: 1.3083, average_loss: 1.7707\n",
            "Epoch [50/80], Iter [100/209] Loss: 1.3347, average_loss: 1.8089\n",
            "Epoch [50/80], Iter [150/209] Loss: 1.8093, average_loss: 1.7777\n",
            "Epoch [50/80], Iter [200/209] Loss: 1.9171, average_loss: 1.7832\n",
            "\n",
            "\n",
            "Starting epoch 51 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [51/80], Iter [50/209] Loss: 1.6534, average_loss: 1.7766\n",
            "Epoch [51/80], Iter [100/209] Loss: 1.8091, average_loss: 1.8128\n",
            "Epoch [51/80], Iter [150/209] Loss: 2.0197, average_loss: 1.8316\n",
            "Epoch [51/80], Iter [200/209] Loss: 1.4567, average_loss: 1.8011\n",
            "\n",
            "\n",
            "Starting epoch 52 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [52/80], Iter [50/209] Loss: 1.2404, average_loss: 1.8455\n",
            "Epoch [52/80], Iter [100/209] Loss: 1.9929, average_loss: 1.7925\n",
            "Epoch [52/80], Iter [150/209] Loss: 1.4297, average_loss: 1.7795\n",
            "Epoch [52/80], Iter [200/209] Loss: 1.5619, average_loss: 1.7796\n",
            "\n",
            "\n",
            "Starting epoch 53 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [53/80], Iter [50/209] Loss: 1.5720, average_loss: 1.8155\n",
            "Epoch [53/80], Iter [100/209] Loss: 2.1427, average_loss: 1.8652\n",
            "Epoch [53/80], Iter [150/209] Loss: 2.0196, average_loss: 1.8237\n",
            "Epoch [53/80], Iter [200/209] Loss: 1.2692, average_loss: 1.8015\n",
            "\n",
            "\n",
            "Starting epoch 54 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [54/80], Iter [50/209] Loss: 1.8909, average_loss: 1.7928\n",
            "Epoch [54/80], Iter [100/209] Loss: 1.2937, average_loss: 1.8017\n",
            "Epoch [54/80], Iter [150/209] Loss: 1.8482, average_loss: 1.7842\n",
            "Epoch [54/80], Iter [200/209] Loss: 1.2971, average_loss: 1.8015\n",
            "\n",
            "\n",
            "Starting epoch 55 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [55/80], Iter [50/209] Loss: 1.9245, average_loss: 1.8038\n",
            "Epoch [55/80], Iter [100/209] Loss: 1.7915, average_loss: 1.7830\n",
            "Epoch [55/80], Iter [150/209] Loss: 2.7657, average_loss: 1.7922\n",
            "Epoch [55/80], Iter [200/209] Loss: 1.4653, average_loss: 1.7868\n",
            "\n",
            "\n",
            "Starting epoch 56 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [56/80], Iter [50/209] Loss: 1.8706, average_loss: 1.7921\n",
            "Epoch [56/80], Iter [100/209] Loss: 1.8456, average_loss: 1.7678\n",
            "Epoch [56/80], Iter [150/209] Loss: 1.9378, average_loss: 1.7924\n",
            "Epoch [56/80], Iter [200/209] Loss: 1.9974, average_loss: 1.7853\n",
            "\n",
            "\n",
            "Starting epoch 57 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [57/80], Iter [50/209] Loss: 1.5394, average_loss: 1.8058\n",
            "Epoch [57/80], Iter [100/209] Loss: 2.1669, average_loss: 1.8113\n",
            "Epoch [57/80], Iter [150/209] Loss: 1.9678, average_loss: 1.7945\n",
            "Epoch [57/80], Iter [200/209] Loss: 1.3617, average_loss: 1.7801\n",
            "\n",
            "\n",
            "Starting epoch 58 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [58/80], Iter [50/209] Loss: 1.5066, average_loss: 1.8582\n",
            "Epoch [58/80], Iter [100/209] Loss: 1.7685, average_loss: 1.8070\n",
            "Epoch [58/80], Iter [150/209] Loss: 1.3773, average_loss: 1.7801\n",
            "Epoch [58/80], Iter [200/209] Loss: 2.5512, average_loss: 1.7865\n",
            "\n",
            "\n",
            "Starting epoch 59 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [59/80], Iter [50/209] Loss: 1.4335, average_loss: 1.7696\n",
            "Epoch [59/80], Iter [100/209] Loss: 2.0497, average_loss: 1.7898\n",
            "Epoch [59/80], Iter [150/209] Loss: 1.7191, average_loss: 1.7828\n",
            "Epoch [59/80], Iter [200/209] Loss: 1.9730, average_loss: 1.7629\n",
            "\n",
            "\n",
            "Starting epoch 60 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [60/80], Iter [50/209] Loss: 1.8555, average_loss: 1.7776\n",
            "Epoch [60/80], Iter [100/209] Loss: 1.4437, average_loss: 1.7769\n",
            "Epoch [60/80], Iter [150/209] Loss: 1.8692, average_loss: 1.7804\n",
            "Epoch [60/80], Iter [200/209] Loss: 2.0864, average_loss: 1.7735\n",
            "\n",
            "\n",
            "Starting epoch 61 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [61/80], Iter [50/209] Loss: 1.5486, average_loss: 1.7497\n",
            "Epoch [61/80], Iter [100/209] Loss: 2.2526, average_loss: 1.7378\n",
            "Epoch [61/80], Iter [150/209] Loss: 2.0813, average_loss: 1.7818\n",
            "Epoch [61/80], Iter [200/209] Loss: 2.3612, average_loss: 1.7927\n",
            "\n",
            "\n",
            "Starting epoch 62 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [62/80], Iter [50/209] Loss: 1.4643, average_loss: 1.8044\n",
            "Epoch [62/80], Iter [100/209] Loss: 1.1898, average_loss: 1.7720\n",
            "Epoch [62/80], Iter [150/209] Loss: 1.6851, average_loss: 1.7608\n",
            "Epoch [62/80], Iter [200/209] Loss: 2.1236, average_loss: 1.7682\n",
            "\n",
            "\n",
            "Starting epoch 63 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [63/80], Iter [50/209] Loss: 1.8860, average_loss: 1.8270\n",
            "Epoch [63/80], Iter [100/209] Loss: 1.5787, average_loss: 1.8406\n",
            "Epoch [63/80], Iter [150/209] Loss: 1.6839, average_loss: 1.8024\n",
            "Epoch [63/80], Iter [200/209] Loss: 1.8805, average_loss: 1.7982\n",
            "\n",
            "\n",
            "Starting epoch 64 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [64/80], Iter [50/209] Loss: 1.1917, average_loss: 1.7144\n",
            "Epoch [64/80], Iter [100/209] Loss: 1.1781, average_loss: 1.7738\n",
            "Epoch [64/80], Iter [150/209] Loss: 1.7067, average_loss: 1.7650\n",
            "Epoch [64/80], Iter [200/209] Loss: 1.7006, average_loss: 1.7756\n",
            "\n",
            "\n",
            "Starting epoch 65 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [65/80], Iter [50/209] Loss: 1.7204, average_loss: 1.7361\n",
            "Epoch [65/80], Iter [100/209] Loss: 1.0692, average_loss: 1.7363\n",
            "Epoch [65/80], Iter [150/209] Loss: 2.1089, average_loss: 1.7578\n",
            "Epoch [65/80], Iter [200/209] Loss: 2.2169, average_loss: 1.7694\n",
            "\n",
            "\n",
            "Starting epoch 66 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [66/80], Iter [50/209] Loss: 1.6940, average_loss: 1.8811\n",
            "Epoch [66/80], Iter [100/209] Loss: 1.3749, average_loss: 1.7769\n",
            "Epoch [66/80], Iter [150/209] Loss: 2.1766, average_loss: 1.7577\n",
            "Epoch [66/80], Iter [200/209] Loss: 1.9373, average_loss: 1.7598\n",
            "\n",
            "\n",
            "Starting epoch 67 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [67/80], Iter [50/209] Loss: 2.0471, average_loss: 1.7968\n",
            "Epoch [67/80], Iter [100/209] Loss: 1.4133, average_loss: 1.7481\n",
            "Epoch [67/80], Iter [150/209] Loss: 3.0409, average_loss: 1.7774\n",
            "Epoch [67/80], Iter [200/209] Loss: 1.5023, average_loss: 1.7679\n",
            "\n",
            "\n",
            "Starting epoch 68 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [68/80], Iter [50/209] Loss: 1.6482, average_loss: 1.7584\n",
            "Epoch [68/80], Iter [100/209] Loss: 1.9082, average_loss: 1.7845\n",
            "Epoch [68/80], Iter [150/209] Loss: 2.3530, average_loss: 1.7523\n",
            "Epoch [68/80], Iter [200/209] Loss: 1.9351, average_loss: 1.7595\n",
            "\n",
            "\n",
            "Starting epoch 69 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [69/80], Iter [50/209] Loss: 1.6080, average_loss: 1.7558\n",
            "Epoch [69/80], Iter [100/209] Loss: 2.2747, average_loss: 1.7946\n",
            "Epoch [69/80], Iter [150/209] Loss: 1.8738, average_loss: 1.7664\n",
            "Epoch [69/80], Iter [200/209] Loss: 1.8314, average_loss: 1.7681\n",
            "\n",
            "\n",
            "Starting epoch 70 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [70/80], Iter [50/209] Loss: 1.7977, average_loss: 1.7496\n",
            "Epoch [70/80], Iter [100/209] Loss: 1.6996, average_loss: 1.7575\n",
            "Epoch [70/80], Iter [150/209] Loss: 1.9799, average_loss: 1.7474\n",
            "Epoch [70/80], Iter [200/209] Loss: 1.2990, average_loss: 1.7577\n",
            "\n",
            "\n",
            "Starting epoch 71 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [71/80], Iter [50/209] Loss: 1.2773, average_loss: 1.7677\n",
            "Epoch [71/80], Iter [100/209] Loss: 1.4055, average_loss: 1.7459\n",
            "Epoch [71/80], Iter [150/209] Loss: 1.6213, average_loss: 1.7575\n",
            "Epoch [71/80], Iter [200/209] Loss: 2.1232, average_loss: 1.7844\n",
            "\n",
            "\n",
            "Starting epoch 72 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [72/80], Iter [50/209] Loss: 2.2116, average_loss: 1.7467\n",
            "Epoch [72/80], Iter [100/209] Loss: 1.4886, average_loss: 1.7447\n",
            "Epoch [72/80], Iter [150/209] Loss: 1.4202, average_loss: 1.7619\n",
            "Epoch [72/80], Iter [200/209] Loss: 1.3075, average_loss: 1.7645\n",
            "\n",
            "\n",
            "Starting epoch 73 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [73/80], Iter [50/209] Loss: 1.9525, average_loss: 1.8084\n",
            "Epoch [73/80], Iter [100/209] Loss: 1.5936, average_loss: 1.7725\n",
            "Epoch [73/80], Iter [150/209] Loss: 1.4198, average_loss: 1.7642\n",
            "Epoch [73/80], Iter [200/209] Loss: 1.7809, average_loss: 1.7437\n",
            "\n",
            "\n",
            "Starting epoch 74 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [74/80], Iter [50/209] Loss: 2.0914, average_loss: 1.8082\n",
            "Epoch [74/80], Iter [100/209] Loss: 1.4888, average_loss: 1.7759\n",
            "Epoch [74/80], Iter [150/209] Loss: 1.5703, average_loss: 1.7578\n",
            "Epoch [74/80], Iter [200/209] Loss: 1.9274, average_loss: 1.7673\n",
            "\n",
            "\n",
            "Starting epoch 75 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [75/80], Iter [50/209] Loss: 1.0799, average_loss: 1.7462\n",
            "Epoch [75/80], Iter [100/209] Loss: 1.5136, average_loss: 1.7382\n",
            "Epoch [75/80], Iter [150/209] Loss: 1.2616, average_loss: 1.7610\n",
            "Epoch [75/80], Iter [200/209] Loss: 1.7752, average_loss: 1.7691\n",
            "\n",
            "\n",
            "Starting epoch 76 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [76/80], Iter [50/209] Loss: 1.9413, average_loss: 1.7737\n",
            "Epoch [76/80], Iter [100/209] Loss: 1.6709, average_loss: 1.7665\n",
            "Epoch [76/80], Iter [150/209] Loss: 2.5847, average_loss: 1.7533\n",
            "Epoch [76/80], Iter [200/209] Loss: 1.3324, average_loss: 1.7593\n",
            "\n",
            "\n",
            "Starting epoch 77 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [77/80], Iter [50/209] Loss: 1.3692, average_loss: 1.7982\n",
            "Epoch [77/80], Iter [100/209] Loss: 1.9477, average_loss: 1.7574\n",
            "Epoch [77/80], Iter [150/209] Loss: 1.8171, average_loss: 1.7886\n",
            "Epoch [77/80], Iter [200/209] Loss: 1.1874, average_loss: 1.7720\n",
            "\n",
            "\n",
            "Starting epoch 78 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [78/80], Iter [50/209] Loss: 1.5928, average_loss: 1.6856\n",
            "Epoch [78/80], Iter [100/209] Loss: 1.3618, average_loss: 1.7382\n",
            "Epoch [78/80], Iter [150/209] Loss: 1.9474, average_loss: 1.7532\n",
            "Epoch [78/80], Iter [200/209] Loss: 1.7818, average_loss: 1.7428\n",
            "\n",
            "\n",
            "Starting epoch 79 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [79/80], Iter [50/209] Loss: 2.4933, average_loss: 1.6821\n",
            "Epoch [79/80], Iter [100/209] Loss: 1.4178, average_loss: 1.7377\n",
            "Epoch [79/80], Iter [150/209] Loss: 2.1408, average_loss: 1.7732\n",
            "Epoch [79/80], Iter [200/209] Loss: 1.5703, average_loss: 1.7542\n",
            "\n",
            "\n",
            "Starting epoch 80 / 80\n",
            "Learning Rate for this epoch: 1e-05\n",
            "Epoch [80/80], Iter [50/209] Loss: 1.9565, average_loss: 1.7231\n",
            "Epoch [80/80], Iter [100/209] Loss: 2.0287, average_loss: 1.7431\n",
            "Epoch [80/80], Iter [150/209] Loss: 2.3991, average_loss: 1.7361\n",
            "Epoch [80/80], Iter [200/209] Loss: 1.6626, average_loss: 1.7547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VDfLv-9S8_Q"
      },
      "source": [
        "# View example predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdPTPjjYS8_R",
        "outputId": "4a98347a-36af-482e-c264-4a6ab238c629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "net.eval()\n",
        "\n",
        "# select random image from test set\n",
        "image_name = random.choice(test_dataset.fnames)\n",
        "image = cv2.imread(os.path.join(file_root_test, image_name))\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "print('predicting...')\n",
        "result = predict_image(net, image_name, root_img_directory=file_root_test)\n",
        "for left_up, right_bottom, class_name, _, prob in result:\n",
        "    color = COLORS[VOC_CLASSES.index(class_name)]\n",
        "    cv2.rectangle(image, left_up, right_bottom, color, 2)\n",
        "    label = class_name + str(round(prob, 2))\n",
        "    text_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n",
        "    p1 = (left_up[0], left_up[1] - text_size[1])\n",
        "    cv2.rectangle(image, (p1[0] - 2 // 2, p1[1] - 2 - baseline), (p1[0] + text_size[0], p1[1] + text_size[1]),\n",
        "                  color, -1)\n",
        "    cv2.putText(image, label, (p1[0], p1[1] + baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, 8)\n",
        "\n",
        "plt.figure(figsize = (15,15))\n",
        "plt.imshow(image)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1bf857ec0dd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# select random image from test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_root_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "hf24bplfS8_U"
      },
      "source": [
        "## Evaluate on Test\n",
        "\n",
        "To evaluate detection results we use mAP (mean of average precision over each class)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SxIQxZfS8_U",
        "outputId": "02328f5c-0236-4372-c929-4c2f13ae8fcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_aps = evaluate(net, test_dataset_file=annotation_file_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---Evaluate model on test samples---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4950/4950 [27:14<00:00,  3.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---class aeroplane ap 0.43750690921317414---\n",
            "---class bicycle ap 0.6468321373378201---\n",
            "---class bird ap 0.46970808110766804---\n",
            "---class boat ap 0.3508660603131585---\n",
            "---class bottle ap 0.2046604552012521---\n",
            "---class bus ap 0.5994600437100672---\n",
            "---class car ap 0.6602852518890688---\n",
            "---class cat ap 0.7080442221994572---\n",
            "---class chair ap 0.3024152858938073---\n",
            "---class cow ap 0.5363728397520229---\n",
            "---class diningtable ap 0.3233247581946679---\n",
            "---class dog ap 0.6406965091472578---\n",
            "---class horse ap 0.6774364264162505---\n",
            "---class motorbike ap 0.5515599982126405---\n",
            "---class person ap 0.5266081526404972---\n",
            "---class pottedplant ap 0.19071225241677972---\n",
            "---class sheep ap 0.4944018776811659---\n",
            "---class sofa ap 0.4667539702542232---\n",
            "---class train ap 0.5897134512153416---\n",
            "---class tvmonitor ap 0.4512611569956755---\n",
            "---map 0.4914309919895998---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSzh24_LS8_Y"
      },
      "source": [
        "output_submission_csv('my_solution.csv', test_aps)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2lUVd28S8_b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}